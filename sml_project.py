# -*- coding: utf-8 -*-
"""SML-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MgROaPBBtQ3j9SLmi1ZBnChHSNpLzQPa
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import sklearn.preprocessing as skl_pre
import sklearn.linear_model as skl_lm
import sklearn.discriminant_analysis as skl_da
import sklearn.neighbors as skl_nb
import sklearn.model_selection as skl_ms
from sklearn.preprocessing import MinMaxScaler
from sklearn . metrics import classification_report
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
from sklearn.metrics import precision_score, recall_score,f1_score

from sklearn import tree
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
import graphviz
#from IPython.display import set_matplotlib_formats
#set_matplotlib_formats('png')
from IPython.core.pylabtools import figsize
figsize(10, 6) # Width and hight
#plt.style.use('seaborn-white')

! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

# Trainig dataset:
train = pd.read_csv('/content/gdrive/My Drive/SML-mini-proj/training_data.csv')
train.head()

train.describe()

test = pd.read_csv('/content/gdrive/My Drive/SML-mini-proj/songs_to_classify.csv')

"""**Pandas Profiling**"""

from pandas_profiling import ProfileReport
profile = ProfileReport(train, title ='Exploratory Data Analysis', html = {'style': {'full_width': True}})

profile.to_notebook_iframe()

# Create a correlation matrix
corr_metrics = train.corr()
corr_metrics.style.background_gradient()

"""energy and loudness were highly correlated, instead of removing any of these attributes we tried PCA, as we have very less data in training."""

sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00"]
chart = sns.countplot(train.label, palette=HAPPY_COLORS_PALETTE)
plt.title("Number of data in each class")

values = list(train['label'].value_counts()[:14].values)
values

"""**Class 0: contains 298 data points.** 
**Class 1: contains 452 data points.**

**Classes are imbalanced**

### Logistic regression:
"""

X = train.drop(["label"], axis=1)
y = train['label']

# Without normalization + PCA:
n_fold = 5
miss = 0
accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = skl_lm.LogisticRegression(solver = 'liblinear', penalty ='l1', class_weight ='balanced')
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

X = train.drop(["label"], axis=1)
y = train['label']

# With normalization:
scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
print("Explained variance ratio : \n{}".format(pca.explained_variance_ratio_))
print("\n")
print("Number of components = {}".format(pca.n_components_))  
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = skl_lm.LogisticRegression(solver = 'liblinear', penalty ='l1', class_weight ='balanced')
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)



"""### LDA: production accuracy = 73."""

X = train.drop(["label","loudness"], axis=1)
y = train['label']

# Without normalization:

n_fold = 5
miss = 0
accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = skl_da.LinearDiscriminantAnalysis()
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

# With normalization:

X = train.drop(["label"], axis=1)
y = train['label']

scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
print("Explained variance ratio : \n{}".format(pca.explained_variance_ratio_))
print("\n")
print("Number of components = {}".format(pca.n_components_))  
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = skl_da.LinearDiscriminantAnalysis()
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

# Predicting:

X_train = train.drop(["label"], axis=1)
y_train = train['label']
X_test = test

# training dataset:
# With normalization:
scaler = MinMaxScaler()
scaled_X_train = scaler.fit_transform(X_train)
# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X_train)
print("Explained variance ratio : \n{}".format(pca.explained_variance_ratio_))
print("Number of components = {}".format(pca.n_components_))  
X_PCA_train = pca.fit_transform(scaled_X_train)

# test data:
# With normalization:
scaler = MinMaxScaler()
scaled_X_test = scaler.fit_transform(X_test)
# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X_test) 
X_PCA_test = pca.fit_transform(scaled_X_test)

model = skl_da.LinearDiscriminantAnalysis()
model.fit(X_PCA_train,y_train)
prediction = model.predict(X_PCA_test)

print("\n")
print("Prediction for LDA:")
''.join(str(i) for i in prediction)

"""### QDA:"""

X = train.drop(["label","loudness"], axis=1)
y = train['label']

# Without normalization:

n_fold = 5
miss = 0
accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = skl_da.QuadraticDiscriminantAnalysis()
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

X = train.drop(["label"], axis=1)
y = train['label']

# With normalization:
scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
print("Explained variance ratio : \n{}".format(pca.explained_variance_ratio_))
print("\n")
print("Number of components = {}".format(pca.n_components_))  
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = skl_da.QuadraticDiscriminantAnalysis()
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)



"""### KNN: Production accuracy = 72"""

# Without normalization:
X = train.drop(["label","loudness"], axis=1)
y = train['label']

n_fold = 5
cv = skl_ms.KFold(n_splits = n_fold, random_state=2, shuffle = True)
K = np.arange(1,100)
misclassification = np.zeros(len(K))
accuracy = np.zeros(len(K))

for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  for j, k in enumerate(K):
    model = skl_nb.KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    prediction = model.predict(X_val)
    misclassification[j] += np.mean(prediction != y_val)
    accuracy[j] += np.mean(prediction == y_val)
    

misclassification /= n_fold

plt.plot(K, misclassification)
plt.title("K-Fold cross validation for Knn method")
plt.xlabel('k')
plt.ylabel('Validation error')
plt.show()

"""**K = 39, without normalization**"""

# With normalization 

X = train.drop(["label"], axis=1)
y = train["label"]

scaler = MinMaxScaler()
scaled_train = scaler.fit_transform(X)


n_fold = 5

cv = skl_ms.KFold(n_splits = n_fold, random_state=2, shuffle = True)
K = np.arange(1,50)
misclassification = np.zeros(len(K))

for train_index, val_index in cv.split(scaled_train):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

 

  for j, k in enumerate(K):
    model = skl_nb.KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    prediction = model.predict(X_val)
    misclassification[j] += np.mean(prediction != y_val)

misclassification /= n_fold
plt.plot(K, misclassification)
plt.title("K-Fold cross validation for Knn method")
plt.xlabel('k')
plt.ylabel('Validation error')
plt.show()

# With normalization after finding optimum k value as 5

X = train.drop(["label"], axis=1)
y = train["label"]
n_fold = 5

cv = skl_ms.KFold(n_splits = n_fold, random_state=2, shuffle = True)
misclassification = 0

for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  scaler = MinMaxScaler()
  scaled_train = scaler.fit_transform(X_train)
  scaled_val = scaler.fit_transform(X_val)

  model = skl_nb.KNeighborsClassifier(n_neighbors=5)
  model.fit(scaled_train, y_train)
  prediction = model.predict(scaled_val)
  misclassification += np.mean(prediction != y_val)
  print(classification_report(y_val, prediction))

misclassification /= n_fold
print('Misclassification',misclassification)



# With normalization:

X = train.drop(["label"], axis=1)
X = pd.get_dummies(X,columns=['time_signature','key'])
y = train['label']

scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
print("Explained variance ratio : \n{}".format(pca.explained_variance_ratio_))
print("\n")
print("Number of components = {}".format(pca.n_components_))  
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = skl_da.LinearDiscriminantAnalysis()
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)



"""### Decision trees:"""

# With normalization:

X = train.drop(["label"], axis=1)
y = train['label']

scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = tree.DecisionTreeClassifier(max_depth=3, splitter='best',min_samples_split=3,min_samples_leaf=2)
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

model = tree.DecisionTreeClassifier(max_depth=3, splitter='best',min_samples_split=3,min_samples_leaf=2)
model.fit(X, y)

dot_data = tree.export_graphviz(model, out_file=None, feature_names = X.columns, class_names=model.classes_,filled=True,rounded=True, leaves_parallel=True, proportion=True)
graph = graphviz.Source(dot_data)
graph

# Without normalization:

X = train.drop(["label"], axis=1)
y = train['label']

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = tree.DecisionTreeClassifier(max_depth=3, splitter='best',min_samples_split=3,min_samples_leaf=2)
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)



"""### Bagging:"""

# Without normalization:

X = train.drop(["label"], axis=1)
y = train['label']

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = BaggingClassifier(n_estimators=50)
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

# With normalization:

X = train.drop(["label"], axis=1)
y = train['label']

scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = BaggingClassifier(n_estimators=100)
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

"""### Random Forest: Production accuracy = 80%"""

# Without normalization:

X = train.drop(["label"], axis=1)
y = train['label']

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = RandomForestClassifier(n_estimators=200,min_samples_split=8)
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

# With normalization:

X = train.drop(["label"], axis=1)
y = train['label']

scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = RandomForestClassifier(n_estimators=200,min_samples_split=2)
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

# Predicting:

X_train = train.drop(["label"], axis=1)
y_train = train['label']
X_test = test

# training dataset:
# With normalization:
scaler = MinMaxScaler()
scaled_X_train = scaler.fit_transform(X_train)
# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X_train)
print("Explained variance ratio : \n{}".format(pca.explained_variance_ratio_))
print("Number of components = {}".format(pca.n_components_))  
X_PCA_train = pca.fit_transform(scaled_X_train)

# test data:
# With normalization:
scaler = MinMaxScaler()
scaled_X_test = scaler.fit_transform(X_test)
# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X_test) 
X_PCA_test = pca.fit_transform(scaled_X_test)

# Model:
model = RandomForestClassifier(n_estimators=200,min_samples_split=2)
model.fit(X_PCA_train,y_train)
prediction = model.predict(X_PCA_test)

print("\n")
print("Prediction for Random forest:")
''.join(str(i) for i in prediction)

"""#### RandomizedSearchCV:"""

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 20]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
print(random_grid)
{'bootstrap': [True, False],
 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}

# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestClassifier()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X_PCA, y)

rf_random.best_params_

"""### AdaBoost: Production accuracy = 78.5%"""

# Without normalization:

X = train.drop(["label"], axis=1)
y = train['label']

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = AdaBoostClassifier(n_estimators=50)
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

# With normalization:

X = train.drop(["label"], axis=1)
y = train['label']

scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0

accuracy = 0
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = AdaBoostClassifier()
  model.fit(X_train, y_train)
  prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

# Predicting with scaling and PCA:

X_train = train.drop(["label"], axis=1)
y_train = train['label']
X_test = test

# training dataset:
# With normalization:
scaler = MinMaxScaler()
scaled_X_train = scaler.fit_transform(X_train)
# PCA with 99% variance retained.
pca = PCA(.99)
pca.fit(scaled_X_train)
print("Explained variance ratio : \n{}".format(pca.explained_variance_ratio_))
print("Number of components = {}".format(pca.n_components_))  
X_PCA_train = pca.fit_transform(scaled_X_train)

# test data:
# With normalization:
scaler = MinMaxScaler()
scaled_X_test = scaler.fit_transform(X_test)
# PCA with 99% variance retained.
pca = PCA(.99)
pca.fit(scaled_X_test) 
X_PCA_test = pca.fit_transform(scaled_X_test)

# Model:
model = AdaBoostClassifier()
model.fit(X_PCA_train,y_train)
prediction = model.predict(X_PCA_test)

print("\n")
print("Prediction for AdaBoost:")
''.join(str(i) for i in prediction)

# Predicting only scaling:

X_train = train.drop(["label"], axis=1)
y_train = train['label']
X_test = test

# training dataset:
# With normalization:
scaler = MinMaxScaler()
scaled_X_train = scaler.fit_transform(X_train)
# PCA with 95% variance retained.

# test data:
# With normalization:
scaler = MinMaxScaler()
scaled_X_test = scaler.fit_transform(X_test)
# PCA with 95% variance retained.

# Model:
model = AdaBoostClassifier()
model.fit(scaled_X_train,y_train)
prediction = model.predict(scaled_X_test)

print("\n")
print("Prediction for AdaBoost:")
''.join(str(i) for i in prediction)

"""### NN:"""

# Without normalization:

import tensorflow.keras as keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation
from keras.optimizers import Adam
from keras import regularizers

X = np.array(train.drop(['label'], 1))
y = np.array(train['label'])

mean = X.mean(axis=0)
X -= mean
std = X.std(axis=0)
X /= std

from sklearn import model_selection
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y, random_state=42, test_size = 0.2)

def create_binary_model():
    # create model
    model = Sequential()
    model.add(Dense(32, input_dim=13, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(32, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(1, activation='sigmoid'))
    
    # Compile model
    adam = Adam(lr=0.001)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

binary_model = create_binary_model()
print(binary_model.summary())

history=binary_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=10)

# generate classification report using predictions for binary model
from sklearn.metrics import classification_report, accuracy_score
# generate classification report using predictions for binary model 
binary_pred = np.round(binary_model.predict(X_test)).astype(int)

print(accuracy_score(y_test, binary_pred))
print(classification_report(y_test, binary_pred))

# With normalization + PCA:

import tensorflow.keras as keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation
from keras.optimizers import Adam
from keras import regularizers

X = np.array(train.drop(['label'], 1))
y = np.array(train['label'])

scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
X_PCA = pca.fit_transform(scaled_X)

from sklearn import model_selection
X_train, X_test, y_train, y_test = model_selection.train_test_split(X_PCA, y, stratify=y, random_state=42, test_size = 0.2)

def create_binary_model():
    # create model
    model = Sequential()
    model.add(Dense(64, input_dim=9, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(64, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(1, activation='sigmoid'))
    
    # Compile model
    adam = Adam(lr=0.001)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

binary_model = create_binary_model()
print(binary_model.summary())

history=binary_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=10)

# generate classification report using predictions for binary model
from sklearn.metrics import classification_report, accuracy_score
# generate classification report using predictions for binary model 
binary_pred = np.round(binary_model.predict(X_test)).astype(int)

print('Results for Binary Model')
print(accuracy_score(y_test, binary_pred))
print(classification_report(y_test, binary_pred))

# Prediction:

#X_test = np.array(test)

X_test = test
# test data:
# With normalization:
scaler = MinMaxScaler()
scaled_X_test = scaler.fit_transform(X_test)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X_test)
X_PCA = pca.fit_transform(scaled_X_test)

prediction = binary_model.predict(X_PCA)

for i in range(0,200):
  if prediction[i] > 0.5:
    prediction[i] = 1
  else: 
    prediction[i] = 0



print("\n")
print("Prediction for NN:")
''.join(str(i) for i in prediction)

"""#### Trying validation:"""

def create_binary_model():
    # create model
    model = Sequential()
    model.add(Dense(32, input_dim=13, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(32, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(1, activation='sigmoid'))
    
    # Compile model
    adam = Adam(lr=0.001)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

binary_model = create_binary_model()
print(binary_model.summary())

X = train.drop(["label"], axis=1)
y = train['label']

scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0

cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  model = binary_model
  model.fit(X_train, y_train,epochs=50)
  prediction = np.round(model.predict(X_val)).astype(int)
  #miss += np.mean(prediction != y_val)
  #accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

# accuracy /= n_fold
# miss /= n_fold
# print("Missclassification error =", miss)
# print("Accuracy =", accuracy)

"""### Base classifier:"""

# Without Normalization

X = train.drop(["label"], axis=1)
y = train['label']

n_fold = 5
miss = 0
accuracy = 0
prediction = np.ones(150)

cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  # model = AdaBoostClassifier(n_estimators=50)
  # model.fit(X_train, y_train)
  # prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

# With normalization:

X = train.drop(["label"], axis=1)
y = train['label']

scaler = MinMaxScaler()
scaled_X = scaler.fit_transform(X)

# PCA with 95% variance retained.
pca = PCA(.95)
pca.fit(scaled_X)
X_PCA = pca.fit_transform(scaled_X)

n_fold = 5
miss = 0
accuracy = 0
prediction = np.ones(150)

cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)
for train_index, val_index in cv.split(X_PCA):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  # model = BaggingClassifier(n_estimators=100)
  # model.fit(X_train, y_train)
  # prediction = model.predict(X_val)
  miss += np.mean(prediction != y_val)
  accuracy += np.mean(prediction == y_val)
  print(classification_report(y_val, prediction))

accuracy /= n_fold
miss /= n_fold
print("Missclassification error =", miss)
print("Accuracy =", accuracy)

"""## Generating Box plots

"""

# Without normalization + PCA:

X = train.drop(["label"], axis=1)
y = train['label']
n_fold = 5

models = []
models.append(skl_lm.LogisticRegression(solver = 'liblinear', penalty ='l1', class_weight ='balanced'))
models.append(skl_da.LinearDiscriminantAnalysis())
models.append(skl_da.QuadraticDiscriminantAnalysis())
models.append(skl_nb.KNeighborsClassifier(n_neighbors=39))
models.append(tree.DecisionTreeClassifier(max_depth=3, splitter='best',min_samples_split=3,min_samples_leaf=2))
models.append(BaggingClassifier(n_estimators=100))
models.append(RandomForestClassifier(n_estimators=200,min_samples_split=2))
models.append(AdaBoostClassifier(n_estimators=50))
# models.append()

mis = np.zeros((n_fold, len(models)))
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)

for i, (train_index, val_index) in enumerate(cv.split(X)):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  for m in range(np.shape(models)[0]):
    model = models[m]
    model.fit(X_train, y_train)
    prediction = model.predict(X_val)
    mis[i, m] = np.mean(prediction != y_val)

plt.boxplot(mis)
plt.title("5-fold cross validation error for different methods without normalization and PCA")
plt.xticks(np.arange(8)+1, ('Logreg','LDA','QDA','KNN','DecisionTree','Bagging','RandomForest','AdaBoost'))
plt.ylabel('Validation error')
plt.show()

X = train.drop(["label"], axis=1)
y = train['label']

scaler = MinMaxScaler()

# PCA with 95% variance retained.
pca = PCA(.95)

n_fold = 5

models = []
models.append(skl_lm.LogisticRegression(solver = 'liblinear', penalty ='l1', class_weight ='balanced'))
models.append(skl_da.LinearDiscriminantAnalysis())
models.append(skl_da.QuadraticDiscriminantAnalysis())
models.append(skl_nb.KNeighborsClassifier(n_neighbors=7))
models.append(tree.DecisionTreeClassifier(max_depth=3, splitter='best',min_samples_split=3,min_samples_leaf=2))
models.append(BaggingClassifier(n_estimators=100))
models.append(RandomForestClassifier(n_estimators=200,min_samples_split=2))
models.append(AdaBoostClassifier())
# models.append()

mis = np.zeros((n_fold, len(models)))
cv = skl_ms.KFold(n_splits = n_fold, shuffle = True)

for i, (train_index, val_index) in enumerate(cv.split(X)):
  X_train, X_val = X.iloc[train_index], X.iloc[val_index]
  y_train, y_val = y.iloc[train_index], y.iloc[val_index]

  scaled_X_train = scaler.fit_transform(X_train)
  scaled_X_val = scaler.fit_transform(X_val)
  
  for m in range(np.shape(models)[0]):
    model = models[m]
    model.fit(scaled_X_train, y_train)
    prediction = model.predict(scaled_X_val)
    mis[i, m] = np.mean(prediction != y_val)

plt.boxplot(mis)
plt.title("5-fold cross validation error for different methods with normalization and PCA")
plt.xticks(np.arange(8)+1, ('Logreg','LDA','QDA','KNN','DecisionTree','Bagging','RandomForest','AdaBoost'))
plt.ylabel('Validation error')
plt.show()

